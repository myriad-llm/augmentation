{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-02T02:30:58.497450Z",
     "start_time": "2024-11-02T02:28:55.460590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading processed data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "ALL = False\n",
    "NO_SMOTE = True\n",
    "SUB_PROCESSED_DIR = 'processed_62'\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "dtypes = {\n",
    "    'msisdn': 'str',\n",
    "    'start_time': 'str',\n",
    "    'end_time': 'str',\n",
    "    'call_event': 'category',\n",
    "    'other_party': 'str',\n",
    "    'ismultimedia': 'category',\n",
    "    'home_area_code': 'str',\n",
    "    'visit_area_code': 'str',\n",
    "    'called_home_code': 'str',\n",
    "    'called_code': 'str',\n",
    "    'a_serv_type': 'int',\n",
    "    'long_type1': 'int',\n",
    "    'roam_type': 'int',\n",
    "    'a_product_id': 'str',\n",
    "    'open_datetime': 'str',\n",
    "    'call_duration': 'int32',\n",
    "    'cfee': 'float64',\n",
    "    'lfee': 'float64',\n",
    "    'hour': 'int8',\n",
    "    'dayofweek': 'int',\n",
    "    'phone1_type': 'int',\n",
    "    'phone2_type': 'int',\n",
    "    'phone1_loc_city': 'str',\n",
    "    'phone1_loc_province': 'str',\n",
    "    'phone2_loc_city': 'str',\n",
    "    'phone2_loc_province': 'str',\n",
    "    'update_time': 'str',\n",
    "    'date': 'str',\n",
    "    'date_c': 'str'\n",
    "}\n",
    "\n",
    "# 判断 processed 文件夹是否存在\n",
    "import os\n",
    "if not os.path.exists(f'../self_data/{SUB_PROCESSED_DIR}'):\n",
    "    print(\"Creating processed data folder...\")\n",
    "    # 读取CSV文件\n",
    "    labeled_data = pd.read_csv('../self_data/all_trainSet_res.csv', dtype=dtypes)\n",
    "    labels = pd.read_csv('../self_data/all_trainSet_ans.csv', dtype=dtypes)\n",
    "\n",
    "    validation_data = pd.read_csv('../self_data/sorted_validationSet_res_with_head.csv', dtype=dtypes)\n",
    "\n",
    "    # 按照 msisdn 切分 train_data 和 test_data\n",
    "    train_data_msisdn, test_data_msisdn = train_test_split(labels['msisdn'], test_size=TEST_RATIO, random_state=42, stratify=labels['is_sa'])\n",
    "    train_data = labeled_data[labeled_data['msisdn'].isin(train_data_msisdn)]\n",
    "    train_labels = labels[labels['msisdn'].isin(train_data_msisdn)]\n",
    "    assert len(train_data['msisdn'].unique()) == len(train_data_msisdn)\n",
    "\n",
    "    test_data = labeled_data[labeled_data['msisdn'].isin(test_data_msisdn)]\n",
    "    test_labels = labels[labels['msisdn'].isin(test_data_msisdn)]\n",
    "    assert len(test_data['msisdn'].unique()) == len(test_data_msisdn)\n",
    "\n",
    "\n",
    "    # # 遍历 groupby('msisdn') 的结果，对每个 msisdn 进行数据增强\n",
    "    # # ------\n",
    "    from tqdm import tqdm\n",
    "    import os\n",
    "    import sys\n",
    "    from utils.augmentation import Augmentation\n",
    "\n",
    "    addition_train_data = []\n",
    "    addition_train_labels = []\n",
    "\n",
    "    times = 2\n",
    "    ratio_range = 0.1\n",
    "    pbar = tqdm(train_data.groupby('msisdn'))\n",
    "    for msisdn, group in pbar:\n",
    "        if msisdn == 0:\n",
    "            continue\n",
    "        # print(f\"Augmenting msisdn {msisdn}\")\n",
    "        pbar.set_description(f\"Augmenting msisdn {msisdn}\")\n",
    "        label = train_labels[train_labels['msisdn'] == msisdn].iloc[0]['is_sa']\n",
    "        aug = Augmentation(group, label, 'msisdn', 'is_sa')\n",
    "        # 对正负样本进行平衡 样本比 1:4\n",
    "        if label == 1:\n",
    "            res_df, res_labels = aug.times(ratio=ratio_range, times=3+4*times, method='mask')\n",
    "\n",
    "            addition_train_data.append(res_df)\n",
    "            addition_train_labels.append(res_labels)\n",
    "\n",
    "            # res_df, res_labels = aug.times(window_size=100, step_size=80, times=1, method='sliding_window')\n",
    "\n",
    "            # addition_train_data.append(res_df)\n",
    "            # addition_train_labels.append(res_labels)\n",
    "        else:\n",
    "            res_df, res_labels = aug.times(ratio=ratio_range, times=times, method='mask')\n",
    "\n",
    "            addition_train_data.append(res_df)\n",
    "            addition_train_labels.append(res_labels)\n",
    "\n",
    "            # res_df, res_labels = aug.times(window_size=100, step_size=80, times=1, method='sliding_window')\n",
    "\n",
    "            # addition_train_data.append(res_df)\n",
    "            # addition_train_labels.append(res_labels)\n",
    "            \n",
    "    addition_train_data = pd.concat(addition_train_data)\n",
    "    addition_train_labels = pd.concat(addition_train_labels)\n",
    "\n",
    "    # 将新数据加入到train_data中\n",
    "    train_data = pd.concat([train_data, addition_train_data], ignore_index=True).reset_index(drop=True)\n",
    "    train_labels = pd.concat([train_labels, addition_train_labels], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    # 按照 msisdn, start_time 排序\n",
    "    sort_start_time = time.time()\n",
    "    train_data = train_data.sort_values(by=['msisdn', 'start_time']).reset_index(drop=True)\n",
    "    train_labels = train_labels.sort_values(by=['msisdn']).reset_index(drop=True)\n",
    "    print('sort time:', time.time() - sort_start_time)\n",
    "\n",
    "    labels_aug = pd.concat([train_labels, test_labels], ignore_index=True).reindex()\n",
    "    # ------------------\n",
    "\n",
    "    # save\n",
    "    print(\"Saving processed data...\")\n",
    "    os.makedirs(f'../self_data/{SUB_PROCESSED_DIR}', exist_ok=True)\n",
    "    train_data.to_csv(f'../self_data/{SUB_PROCESSED_DIR}/train_data.csv', index=False)\n",
    "    train_labels.to_csv(f'../self_data/{SUB_PROCESSED_DIR}/train_labels.csv', index=False)\n",
    "    test_data.to_csv(f'../self_data/{SUB_PROCESSED_DIR}/test_data.csv', index=False)\n",
    "    test_labels.to_csv(f'../self_data/{SUB_PROCESSED_DIR}/test_labels.csv', index=False)\n",
    "\n",
    "    labels_aug.to_csv(f'../self_data/{SUB_PROCESSED_DIR}/labels_aug.csv', index=False)\n",
    "\n",
    "    validation_data.to_csv(f'../self_data/{SUB_PROCESSED_DIR}/validation_data.csv', index=False)\n",
    "    # TODO: test_data\n",
    "\n",
    "else:\n",
    "    print(\"Reading processed data...\")\n",
    "    train_data = pd.read_csv(f'../self_data/{SUB_PROCESSED_DIR}/train_data.csv', dtype=dtypes)\n",
    "    train_labels = pd.read_csv(f'../self_data/{SUB_PROCESSED_DIR}/train_labels.csv', dtype=dtypes)\n",
    "    test_data = pd.read_csv(f'../self_data/{SUB_PROCESSED_DIR}/test_data.csv', dtype=dtypes)\n",
    "    test_labels = pd.read_csv(f'../self_data/{SUB_PROCESSED_DIR}/test_labels.csv', dtype=dtypes)\n",
    "\n",
    "    labels_aug = pd.read_csv(f'../self_data/{SUB_PROCESSED_DIR}/labels_aug.csv', dtype=dtypes)\n",
    "\n",
    "    validation_data = pd.read_csv(f'../self_data/{SUB_PROCESSED_DIR}/validation_data.csv', dtype=dtypes)\n",
    "\n",
    "labeled_data_aug = pd.concat([train_data, test_data], ignore_index=True).reindex()\n",
    "assert len(labeled_data_aug['msisdn'].unique()) == len(labels_aug['msisdn'].unique())\n",
    "\n",
    "# 转换时间格式\n",
    "labeled_data_aug['start_time'] = pd.to_datetime(labeled_data_aug['start_time'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['end_time'] = pd.to_datetime(labeled_data_aug['end_time'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['open_datetime'] = pd.to_datetime(labeled_data_aug['open_datetime'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['update_time'] = pd.to_datetime(labeled_data_aug['update_time'])\n",
    "labeled_data_aug['date'] = pd.to_datetime(labeled_data_aug['date'])\n",
    "\n",
    "validation_data['start_time'] = pd.to_datetime(validation_data['start_time'], format='%Y%m%d%H%M%S')\n",
    "validation_data['end_time'] = pd.to_datetime(validation_data['end_time'], format='%Y%m%d%H%M%S')\n",
    "validation_data['open_datetime'] = pd.to_datetime(validation_data['open_datetime'], format='%Y%m%d%H%M%S',errors='coerce')\n",
    "validation_data['update_time'] = pd.to_datetime(validation_data['update_time'])\n",
    "validation_data['date'] = pd.to_datetime(validation_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:31:00.292045Z",
     "start_time": "2024-11-02T02:30:58.515478Z"
    }
   },
   "outputs": [],
   "source": [
    "# 为每条记录添加start_time_diff，记录 start_time 与上一条记录的 start_time 之差 (单位：秒)\n",
    "start_time_diff = labeled_data_aug.groupby('msisdn')['start_time'].diff().dt.total_seconds().fillna(0).reset_index(drop=True)\n",
    "# 将该列加入到数据集中\n",
    "labeled_data_aug['start_time_diff'] = start_time_diff.copy()\n",
    "# time_diff_start2end = train_data.groupby('msisdn')['end_time'].diff().dt.total_seconds().fillna(0)\n",
    "start_time_diff = validation_data.groupby('msisdn')['start_time'].diff().dt.total_seconds().fillna(0).reset_index(drop=True)\n",
    "validation_data['start_time_diff'] = start_time_diff.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:03.693173Z",
     "start_time": "2024-11-02T02:31:00.296205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_10300\\1588848855.py:46: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ('called_diff_home_code', lambda x: x[x != x.shift()].count() / x.count())\n",
      "C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_10300\\1588848855.py:50: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ('diff', lambda x: x[x != x.shift()].count()/ x.count())\n"
     ]
    }
   ],
   "source": [
    "# 聚合特征\n",
    "def aggregate_features(data):\n",
    "    return data.groupby('msisdn').agg({\n",
    "    'call_duration': [\n",
    "        ('sum', 'sum'), \n",
    "        ('mean', 'mean'), \n",
    "        ('max', 'max'), \n",
    "        ('std', 'std'),\n",
    "        ('quantile_25', lambda x: x.quantile(0.25)), \n",
    "        ('quantile_50', lambda x: x.quantile(0.50)), \n",
    "        ('quantile_75', lambda x: x.quantile(0.75)),\n",
    "    ],\n",
    "    'cfee': [\n",
    "        ('sum', 'sum'),\n",
    "        ('std', 'std'), \n",
    "        ('mean', 'mean'),\n",
    "    ],\n",
    "    'lfee': [\n",
    "        ('sum', 'sum'), \n",
    "        ('mean', 'mean'),\n",
    "        ('std', 'std'),\n",
    "    ],\n",
    "    'hour': [\n",
    "        ('mean', 'mean'), \n",
    "        ('std', 'std'), \n",
    "        ('max', 'max'), \n",
    "        ('min', 'min'),\n",
    "    ],\n",
    "    'dayofweek': [\n",
    "        ('std', 'std'), \n",
    "        ('magic', lambda x: x.value_counts().mean()), \n",
    "        ('work_day_num', lambda x: x[x.isin([1,2,3,4,5])].count()), \n",
    "        ('weekend_num', lambda x: x[x.isin([6,7])].count()),\n",
    "        ('mode', lambda x: x.mode().values[0]),\n",
    "        ('work_day_weekend_diff', lambda x: (x[x.isin([1,2,3,4,5])].count() - x[x.isin([6,7])].count()) / (x[x.isin([1,2,3,4,5])].count() + x[x.isin([6,7])].count())),\n",
    "    ],\n",
    "    # 'home_area_code': [\n",
    "    #     ('home_area_code_nunique', 'nunique')\n",
    "    # ],\n",
    "    'visit_area_code': [\n",
    "        ('nunique', 'nunique'),\n",
    "        ('times_not_at_home_area', lambda x: x[x != x.shift()].count()/x.count())\n",
    "    ],\n",
    "    'called_home_code': [\n",
    "        ('nunique', 'nunique'),\n",
    "        ('called_diff_home_code', lambda x: x[x != x.shift()].count() / x.count())\n",
    "    ],\n",
    "    'called_code': [\n",
    "        ('nunique', 'nunique'),\n",
    "        ('diff', lambda x: x[x != x.shift()].count()/ x.count())\n",
    "    ],\n",
    "    'open_datetime': [\n",
    "        ('open_count', 'nunique')\n",
    "    ],\n",
    "    'other_party': [\n",
    "        ('account_person_num', 'nunique'),\n",
    "        ('called_diff_home_code', lambda x: x[x != x.shift()].count() / x.count())\n",
    "    ],\n",
    "    'a_serv_type': [\n",
    "        ('call_num', lambda x: x[x.isin([1, 3])].count()), \n",
    "        ('called_num', lambda x: x[x == 2].count()),\n",
    "        ('call_called_normalized_diff', lambda x: (x[x.isin([1, 3])].count() - x[x == 2].count()) /  (x[x.isin([1, 3])].count() + x[x == 2].count())),\n",
    "    ],\n",
    "    'start_time_diff': [\n",
    "        ('start_time_diff_mean', 'mean'), \n",
    "        ('start_time_diff_std', 'std'), \n",
    "        ('max', 'max'), \n",
    "        ('coefficient_of_variation', lambda x: x.std() / x.mean()),\n",
    "    ], \n",
    "    # 'phone1_type': [\n",
    "    #     ('nunique', 'nunique'),\n",
    "    #     ('mode', lambda x: x.mode().values[0])\n",
    "    # ],\n",
    "    # 'distance': [\n",
    "    #     ('sum', 'sum'), \n",
    "    #     ('std', 'std'), \n",
    "    #     ('max', 'max'), \n",
    "    #     ('quantile_25', lambda x: x.quantile(0.25)), \n",
    "    #     ('quantile_50', lambda x: x.quantile(0.50)), \n",
    "    #     ('quantile_75', lambda x: x.quantile(0.75)),\n",
    "    # ]\n",
    "})\n",
    "\n",
    "labeled_aug_features = aggregate_features(labeled_data_aug)\n",
    "validation_features = aggregate_features(validation_data)\n",
    "\n",
    "labeled_aug_features.columns = ['+'.join(col).strip() for col in labeled_aug_features.columns.values]\n",
    "validation_features.columns = ['+'.join(col).strip() for col in validation_features.columns.values]\n",
    "\n",
    "labeled_aug_features.columns = labeled_aug_features.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '').str.replace('>', '').str.replace('(', '').str.replace(')', '').str.replace(',', '').str.replace(' ', '_')\n",
    "validation_features.columns = validation_features.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '').str.replace('>', '').str.replace('(', '').str.replace(')', '').str.replace(',', '').str.replace(' ', '_')\n",
    "\n",
    "# 重置索引\n",
    "labeled_aug_features = labeled_aug_features.reset_index()\n",
    "validation_features = validation_features.reset_index()\n",
    "\n",
    "# 合并标签数据\n",
    "labeled_aug_features = labeled_aug_features.merge(labels_aug, on='msisdn', how='left')\n",
    "# 打印结果\n",
    "# labeled_aug_features\n",
    "\n",
    "# # 添加 ae 的编码特征\n",
    "# labeled_ae = pd.read_csv('../data/ae/train.csv', dtype=dtypes)\n",
    "# valid_ae = pd.read_csv('../data/ae/val.csv', dtype=dtypes)\n",
    "# labeled_aug_features = labeled_aug_features.merge(labeled_ae, on='msisdn', how='left')\n",
    "# validation_features = validation_features.merge(valid_ae, on='msisdn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:03.709035Z",
     "start_time": "2024-11-02T02:42:03.694441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['msisdn', 'call_duration+sum', 'call_duration+mean',\n       'call_duration+max', 'call_duration+std', 'call_duration+quantile_25',\n       'call_duration+quantile_50', 'call_duration+quantile_75', 'cfee+sum',\n       'cfee+std', 'cfee+mean', 'lfee+sum', 'lfee+mean', 'lfee+std',\n       'hour+mean', 'hour+std', 'hour+max', 'hour+min', 'dayofweek+std',\n       'dayofweek+magic', 'dayofweek+work_day_num', 'dayofweek+weekend_num',\n       'dayofweek+mode', 'dayofweek+work_day_weekend_diff',\n       'visit_area_code+nunique', 'visit_area_code+times_not_at_home_area',\n       'called_home_code+nunique', 'called_home_code+called_diff_home_code',\n       'called_code+nunique', 'called_code+diff', 'open_datetime+open_count',\n       'other_party+account_person_num', 'other_party+called_diff_home_code',\n       'a_serv_type+call_num', 'a_serv_type+called_num',\n       'a_serv_type+call_called_normalized_diff',\n       'start_time_diff+start_time_diff_mean',\n       'start_time_diff+start_time_diff_std', 'start_time_diff+max',\n       'start_time_diff+coefficient_of_variation', 'is_sa'],\n      dtype='object')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_aug_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:03.724314Z",
     "start_time": "2024-11-02T02:42:03.711286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "41"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_aug_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:03.800902Z",
     "start_time": "2024-11-02T02:42:03.725841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_duration+std 146\n",
      "cfee+std 146\n",
      "lfee+std 146\n",
      "hour+std 146\n",
      "dayofweek+std 146\n",
      "called_home_code+called_diff_home_code 1\n",
      "called_code+diff 1\n",
      "start_time_diff+start_time_diff_std 146\n",
      "start_time_diff+coefficient_of_variation 146\n",
      "call_duration+std 78\n",
      "cfee+std 78\n",
      "lfee+std 78\n",
      "hour+std 78\n",
      "dayofweek+std 78\n",
      "start_time_diff+start_time_diff_std 78\n",
      "start_time_diff+coefficient_of_variation 78\n"
     ]
    },
    {
     "data": {
      "text/plain": "(         msisdn  call_duration+sum  call_duration+mean  call_duration+max  \\\n 330     1005436                 25                25.0                 25   \n 945     1010686                 16                16.0                 16   \n 1755    1013277                351               351.0                351   \n 1950    1013990                 56                56.0                 56   \n 2127    1015735                 66                66.0                 66   \n ...         ...                ...                 ...                ...   \n 128202  2419249                  2                 2.0                  2   \n 129292  2423456                 14                14.0                 14   \n 132326  2500562                 23                23.0                 23   \n 132693  2502357                 74                74.0                 74   \n 132764  2502602                507               507.0                507   \n \n         call_duration+std  call_duration+quantile_25  \\\n 330                   NaN                       25.0   \n 945                   NaN                       16.0   \n 1755                  NaN                      351.0   \n 1950                  NaN                       56.0   \n 2127                  NaN                       66.0   \n ...                   ...                        ...   \n 128202                NaN                        2.0   \n 129292                NaN                       14.0   \n 132326                NaN                       23.0   \n 132693                NaN                       74.0   \n 132764                NaN                      507.0   \n \n         call_duration+quantile_50  call_duration+quantile_75  cfee+sum  \\\n 330                          25.0                       25.0       0.0   \n 945                          16.0                       16.0       0.0   \n 1755                        351.0                      351.0       0.0   \n 1950                         56.0                       56.0       0.0   \n 2127                         66.0                       66.0       0.0   \n ...                           ...                        ...       ...   \n 128202                        2.0                        2.0       0.0   \n 129292                       14.0                       14.0       0.0   \n 132326                       23.0                       23.0       0.0   \n 132693                       74.0                       74.0       0.0   \n 132764                      507.0                      507.0       0.0   \n \n         cfee+std  ...  other_party+account_person_num  \\\n 330          NaN  ...                               1   \n 945          NaN  ...                               1   \n 1755         NaN  ...                               1   \n 1950         NaN  ...                               1   \n 2127         NaN  ...                               1   \n ...          ...  ...                             ...   \n 128202       NaN  ...                               1   \n 129292       NaN  ...                               1   \n 132326       NaN  ...                               1   \n 132693       NaN  ...                               1   \n 132764       NaN  ...                               1   \n \n         other_party+called_diff_home_code  a_serv_type+call_num  \\\n 330                                   1.0                     0   \n 945                                   1.0                     1   \n 1755                                  1.0                     1   \n 1950                                  1.0                     0   \n 2127                                  1.0                     1   \n ...                                   ...                   ...   \n 128202                                1.0                     0   \n 129292                                1.0                     0   \n 132326                                1.0                     1   \n 132693                                1.0                     0   \n 132764                                1.0                     1   \n \n         a_serv_type+called_num  a_serv_type+call_called_normalized_diff  \\\n 330                          1                                     -1.0   \n 945                          0                                      1.0   \n 1755                         0                                      1.0   \n 1950                         1                                     -1.0   \n 2127                         0                                      1.0   \n ...                        ...                                      ...   \n 128202                       1                                     -1.0   \n 129292                       1                                     -1.0   \n 132326                       0                                      1.0   \n 132693                       1                                     -1.0   \n 132764                       0                                      1.0   \n \n         start_time_diff+start_time_diff_mean  \\\n 330                                      0.0   \n 945                                      0.0   \n 1755                                     0.0   \n 1950                                     0.0   \n 2127                                     0.0   \n ...                                      ...   \n 128202                                   0.0   \n 129292                                   0.0   \n 132326                                   0.0   \n 132693                                   0.0   \n 132764                                   0.0   \n \n         start_time_diff+start_time_diff_std  start_time_diff+max  \\\n 330                                     NaN                  0.0   \n 945                                     NaN                  0.0   \n 1755                                    NaN                  0.0   \n 1950                                    NaN                  0.0   \n 2127                                    NaN                  0.0   \n ...                                     ...                  ...   \n 128202                                  NaN                  0.0   \n 129292                                  NaN                  0.0   \n 132326                                  NaN                  0.0   \n 132693                                  NaN                  0.0   \n 132764                                  NaN                  0.0   \n \n         start_time_diff+coefficient_of_variation  is_sa  \n 330                                          NaN      0  \n 945                                          NaN      1  \n 1755                                         NaN      1  \n 1950                                         NaN      1  \n 2127                                         NaN      1  \n ...                                          ...    ...  \n 128202                                       NaN      0  \n 129292                                       NaN      0  \n 132326                                       NaN      0  \n 132693                                       NaN      0  \n 132764                                       NaN      0  \n \n [146 rows x 41 columns],\n         msisdn  call_duration+sum  call_duration+mean  call_duration+max  \\\n 546    1032602                 45                45.0                 45   \n 713    1076005                 47                47.0                 47   \n 779    1078136                319               319.0                319   \n 962    1084780                 12                12.0                 12   \n 1180   1096682                 40                40.0                 40   \n ...        ...                ...                 ...                ...   \n 12160  2374034                338               338.0                338   \n 12245  2380179                271               271.0                271   \n 12326  2382553                200               200.0                200   \n 12510  2423043                 30                30.0                 30   \n 12863  2501905                 13                13.0                 13   \n \n        call_duration+std  call_duration+quantile_25  \\\n 546                  NaN                       45.0   \n 713                  NaN                       47.0   \n 779                  NaN                      319.0   \n 962                  NaN                       12.0   \n 1180                 NaN                       40.0   \n ...                  ...                        ...   \n 12160                NaN                      338.0   \n 12245                NaN                      271.0   \n 12326                NaN                      200.0   \n 12510                NaN                       30.0   \n 12863                NaN                       13.0   \n \n        call_duration+quantile_50  call_duration+quantile_75  cfee+sum  \\\n 546                         45.0                       45.0       0.0   \n 713                         47.0                       47.0       0.0   \n 779                        319.0                      319.0       0.0   \n 962                         12.0                       12.0       0.0   \n 1180                        40.0                       40.0       0.0   \n ...                          ...                        ...       ...   \n 12160                      338.0                      338.0       0.0   \n 12245                      271.0                      271.0       0.0   \n 12326                      200.0                      200.0       0.0   \n 12510                       30.0                       30.0       0.0   \n 12863                       13.0                       13.0       0.0   \n \n        cfee+std  ...  open_datetime+open_count  \\\n 546         NaN  ...                         1   \n 713         NaN  ...                         1   \n 779         NaN  ...                         1   \n 962         NaN  ...                         1   \n 1180        NaN  ...                         1   \n ...         ...  ...                       ...   \n 12160       NaN  ...                         1   \n 12245       NaN  ...                         1   \n 12326       NaN  ...                         1   \n 12510       NaN  ...                         1   \n 12863       NaN  ...                         1   \n \n        other_party+account_person_num  other_party+called_diff_home_code  \\\n 546                                 1                                1.0   \n 713                                 1                                1.0   \n 779                                 1                                1.0   \n 962                                 1                                1.0   \n 1180                                1                                1.0   \n ...                               ...                                ...   \n 12160                               1                                1.0   \n 12245                               1                                1.0   \n 12326                               1                                1.0   \n 12510                               1                                1.0   \n 12863                               1                                1.0   \n \n        a_serv_type+call_num  a_serv_type+called_num  \\\n 546                       0                       1   \n 713                       1                       0   \n 779                       1                       0   \n 962                       1                       0   \n 1180                      1                       0   \n ...                     ...                     ...   \n 12160                     1                       0   \n 12245                     1                       0   \n 12326                     1                       0   \n 12510                     1                       0   \n 12863                     0                       1   \n \n        a_serv_type+call_called_normalized_diff  \\\n 546                                       -1.0   \n 713                                        1.0   \n 779                                        1.0   \n 962                                        1.0   \n 1180                                       1.0   \n ...                                        ...   \n 12160                                      1.0   \n 12245                                      1.0   \n 12326                                      1.0   \n 12510                                      1.0   \n 12863                                     -1.0   \n \n        start_time_diff+start_time_diff_mean  \\\n 546                                     0.0   \n 713                                     0.0   \n 779                                     0.0   \n 962                                     0.0   \n 1180                                    0.0   \n ...                                     ...   \n 12160                                   0.0   \n 12245                                   0.0   \n 12326                                   0.0   \n 12510                                   0.0   \n 12863                                   0.0   \n \n        start_time_diff+start_time_diff_std  start_time_diff+max  \\\n 546                                    NaN                  0.0   \n 713                                    NaN                  0.0   \n 779                                    NaN                  0.0   \n 962                                    NaN                  0.0   \n 1180                                   NaN                  0.0   \n ...                                    ...                  ...   \n 12160                                  NaN                  0.0   \n 12245                                  NaN                  0.0   \n 12326                                  NaN                  0.0   \n 12510                                  NaN                  0.0   \n 12863                                  NaN                  0.0   \n \n        start_time_diff+coefficient_of_variation  \n 546                                         NaN  \n 713                                         NaN  \n 779                                         NaN  \n 962                                         NaN  \n 1180                                        NaN  \n ...                                         ...  \n 12160                                       NaN  \n 12245                                       NaN  \n 12326                                       NaN  \n 12510                                       NaN  \n 12863                                       NaN  \n \n [78 rows x 40 columns])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nan(train):\n",
    "    # 获取 train 中的 nan值\n",
    "    train_nan = train[train.isnull().T.any()]\n",
    "    # 统计 每列含有的 nan 数量\n",
    "    for col in train.columns:\n",
    "        if train[col].isnull().sum() > 0:\n",
    "            print(col, train[col].isnull().sum())\n",
    "\n",
    "    return train_nan\n",
    "get_nan(labeled_aug_features), get_nan(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:03.878665Z",
     "start_time": "2024-11-02T02:42:03.805426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(Empty DataFrame\n Columns: [msisdn, call_duration+sum, call_duration+mean, call_duration+max, call_duration+std, call_duration+quantile_25, call_duration+quantile_50, call_duration+quantile_75, cfee+sum, cfee+std, cfee+mean, lfee+sum, lfee+mean, lfee+std, hour+mean, hour+std, hour+max, hour+min, dayofweek+std, dayofweek+magic, dayofweek+work_day_num, dayofweek+weekend_num, dayofweek+mode, dayofweek+work_day_weekend_diff, visit_area_code+nunique, visit_area_code+times_not_at_home_area, called_home_code+nunique, called_home_code+called_diff_home_code, called_code+nunique, called_code+diff, open_datetime+open_count, other_party+account_person_num, other_party+called_diff_home_code, a_serv_type+call_num, a_serv_type+called_num, a_serv_type+call_called_normalized_diff, start_time_diff+start_time_diff_mean, start_time_diff+start_time_diff_std, start_time_diff+max, start_time_diff+coefficient_of_variation, is_sa]\n Index: []\n \n [0 rows x 41 columns],\n Empty DataFrame\n Columns: [msisdn, call_duration+sum, call_duration+mean, call_duration+max, call_duration+std, call_duration+quantile_25, call_duration+quantile_50, call_duration+quantile_75, cfee+sum, cfee+std, cfee+mean, lfee+sum, lfee+mean, lfee+std, hour+mean, hour+std, hour+max, hour+min, dayofweek+std, dayofweek+magic, dayofweek+work_day_num, dayofweek+weekend_num, dayofweek+mode, dayofweek+work_day_weekend_diff, visit_area_code+nunique, visit_area_code+times_not_at_home_area, called_home_code+nunique, called_home_code+called_diff_home_code, called_code+nunique, called_code+diff, open_datetime+open_count, other_party+account_person_num, other_party+called_diff_home_code, a_serv_type+call_num, a_serv_type+called_num, a_serv_type+call_called_normalized_diff, start_time_diff+start_time_diff_mean, start_time_diff+start_time_diff_std, start_time_diff+max, start_time_diff+coefficient_of_variation]\n Index: []\n \n [0 rows x 40 columns])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一般只有 std 会出现 nan 值故所有的 nan 值填充为 0\n",
    "labeled_aug_features = labeled_aug_features.fillna(0)\n",
    "validation_features = validation_features.fillna(0)\n",
    "\n",
    "def get_nan(train):\n",
    "    # 获取 train 中的 nan值\n",
    "    train_nan = train[train.isnull().T.any()]\n",
    "    # 统计 每列含有的 nan 数量\n",
    "    for col in train.columns:\n",
    "        if train[col].isnull().sum() > 0:\n",
    "            print(col, train[col].isnull().sum())\n",
    "\n",
    "    return train_nan\n",
    "get_nan(labeled_aug_features), get_nan(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 交叉特征\n",
    "# # # 将所有特征两两相乘\n",
    "# from itertools import combinations\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def cross_features(data):\n",
    "#     cross_features = []\n",
    "#     new_features = []\n",
    "#     cross_cols = data.columns.tolist()\n",
    "#     rm_cols = ['msisdn', 'is_sa']\n",
    "#     for col in rm_cols:\n",
    "#         if col in cross_cols:\n",
    "#             cross_cols.remove(col)\n",
    "\n",
    "#     for i, j in tqdm(combinations(cross_cols, 2), total=len(cross_cols) * (len(cross_cols) - 1) // 2):\n",
    "#         new_features.append(data[i] * data[j])\n",
    "#         cross_features.append(f'{i}_cross_{j}')\n",
    "#     new_features = pd.concat(new_features, axis=1)\n",
    "#     new_features.columns = cross_features\n",
    "#     data = pd.concat([data, new_features], axis=1)\n",
    "#     return data, cross_features\n",
    "\n",
    "# labeled_aug_features, _ = cross_features(labeled_aug_features)\n",
    "# validation_features, _ = cross_features(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:03.909856Z",
     "start_time": "2024-11-02T02:42:03.880670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本个数：133591; 正样本占29.31%; 负样本占70.69%\n",
      "特征维数： 40\n"
     ]
    }
   ],
   "source": [
    "X = labeled_aug_features.drop(['msisdn'], axis=1)\n",
    "y = labeled_aug_features['is_sa']\n",
    "X_validation = validation_features.drop(['msisdn'], axis=1)\n",
    "\n",
    "n_sample = y.shape[0]\n",
    "n_pos_sample = y[y ==1].shape[0]\n",
    "n_neg_sample = y[y == 0].shape[0]\n",
    "print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('特征维数：', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:03.925380Z",
     "start_time": "2024-11-02T02:42:03.911858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['msisdn', 'call_duration+sum', 'call_duration+mean',\n       'call_duration+max', 'call_duration+std', 'call_duration+quantile_25',\n       'call_duration+quantile_50', 'call_duration+quantile_75', 'cfee+sum',\n       'cfee+std', 'cfee+mean', 'lfee+sum', 'lfee+mean', 'lfee+std',\n       'hour+mean', 'hour+std', 'hour+max', 'hour+min', 'dayofweek+std',\n       'dayofweek+magic', 'dayofweek+work_day_num', 'dayofweek+weekend_num',\n       'dayofweek+mode', 'dayofweek+work_day_weekend_diff',\n       'visit_area_code+nunique', 'visit_area_code+times_not_at_home_area',\n       'called_home_code+nunique', 'called_home_code+called_diff_home_code',\n       'called_code+nunique', 'called_code+diff', 'open_datetime+open_count',\n       'other_party+account_person_num', 'other_party+called_diff_home_code',\n       'a_serv_type+call_num', 'a_serv_type+called_num',\n       'a_serv_type+call_called_normalized_diff',\n       'start_time_diff+start_time_diff_mean',\n       'start_time_diff+start_time_diff_std', 'start_time_diff+max',\n       'start_time_diff+coefficient_of_variation', 'is_sa'],\n      dtype='object')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_aug_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.268299Z",
     "start_time": "2024-11-02T02:42:03.926887Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO use all_X to impute\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.314574Z",
     "start_time": "2024-11-02T02:42:04.270215Z"
    }
   },
   "outputs": [],
   "source": [
    "imputer2 = SimpleImputer(strategy='most_frequent')\n",
    "X_validation = imputer2.fit_transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.330093Z",
     "start_time": "2024-11-02T02:42:04.316574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(133591, 40)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.345637Z",
     "start_time": "2024-11-02T02:42:04.332099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(13005, 39)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.360743Z",
     "start_time": "2024-11-02T02:42:04.346638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(133591,)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.375949Z",
     "start_time": "2024-11-02T02:42:04.362334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(129732, 3859)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(train_labels) + len(test_labels) == len(labeled_aug_features)\n",
    "len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.531207Z",
     "start_time": "2024-11-02T02:42:04.378043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 samples / 0 samples in train set: 38768 / 90964\n",
      "1 samples / 0 samples in test set: 384 / 3475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# 将 msisdn 和 is_sa 并入 X 再划分\n",
    "train_data_msisdn = train_labels['msisdn']\n",
    "test_data_msisdn = test_labels['msisdn']\n",
    "X_df = pd.DataFrame(X, columns=labeled_aug_features.drop(['msisdn'], axis=1).columns)\n",
    "X_df = pd.concat([labeled_aug_features[['msisdn']], X_df], axis=1)\n",
    "train_set = X_df[X_df['msisdn'].isin(train_data_msisdn)][X_df.columns[1:]]\n",
    "test_set = X_df[X_df['msisdn'].isin(test_data_msisdn)][X_df.columns[1:]]\n",
    "\n",
    "print(f\"1 samples / 0 samples in train set: {len(train_set[train_set['is_sa'] == 1])} / {len(train_set[train_set['is_sa'] == 0])}\")\n",
    "print(f\"1 samples / 0 samples in test set: {len(test_set[test_set['is_sa'] == 1])} / {len(test_set[test_set['is_sa'] == 0])}\")\n",
    "\n",
    "if ALL:\n",
    "    # if not NO_SMOTE:\n",
    "    #     smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "    #     X, y = smote.fit_resample(X, y)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    train_len = len(test_set) + len(train_set)\n",
    "    test_len = 0\n",
    "else:\n",
    "    # X_train,X_test,y_train,y_test = train_test_split(X,y,stratify = y,test_size= 0.3,random_state=42, shuffle=True)\n",
    "    # X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "    train_len, test_len = len(train_set), len(test_set)\n",
    "\n",
    "    # if not NO_SMOTE:\n",
    "    #     smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "    #     X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    #     print('通过SMOTE方法平衡正负样本后')\n",
    "    #     n_sample = y_train.shape[0]\n",
    "    #     n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "    #     n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "    #     print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "    #                                                     n_pos_sample / n_sample,\n",
    "    #                                                     n_neg_sample / n_sample))\n",
    "    #     print('特征维数：', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.546964Z",
     "start_time": "2024-11-02T02:42:04.532439Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = labeled_aug_features.columns.tolist()\n",
    "columns.remove('msisdn')\n",
    "valid_set = np.c_[X_validation, np.zeros(X_validation.shape[0])]\n",
    "valid_set = pd.DataFrame(valid_set, columns=columns)\n",
    "valid_set['is_sa'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.577486Z",
     "start_time": "2024-11-02T02:42:04.549406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   call_duration+sum  call_duration+mean  call_duration+max  \\\n0             6369.0           68.483871             1129.0   \n1             6127.0           72.940476             1129.0   \n2             6067.0           72.226190             1129.0   \n3              375.0           20.833333              156.0   \n4              370.0           21.764706              156.0   \n\n   call_duration+std  call_duration+quantile_25  call_duration+quantile_50  \\\n0         128.481906                      20.00                       37.0   \n1         134.267612                      21.75                       41.0   \n2         134.270752                      21.75                       41.0   \n3          39.165186                       3.50                        8.0   \n4          40.164551                       3.00                        8.0   \n\n   call_duration+quantile_75  cfee+sum  cfee+std  cfee+mean  ...  \\\n0                       78.0       0.0       0.0        0.0  ...   \n1                       78.5       0.0       0.0        0.0  ...   \n2                       78.5       0.0       0.0        0.0  ...   \n3                       13.5       0.0       0.0        0.0  ...   \n4                       15.0       0.0       0.0        0.0  ...   \n\n   other_party+account_person_num  other_party+called_diff_home_code  \\\n0                            44.0                           0.860215   \n1                            40.0                           0.869048   \n2                            41.0                           0.869048   \n3                             6.0                           0.333333   \n4                             6.0                           0.352941   \n\n   a_serv_type+call_num  a_serv_type+called_num  \\\n0                  62.0                    31.0   \n1                  55.0                    29.0   \n2                  55.0                    29.0   \n3                  13.0                     5.0   \n4                  13.0                     4.0   \n\n   a_serv_type+call_called_normalized_diff  \\\n0                                 0.333333   \n1                                 0.309524   \n2                                 0.309524   \n3                                 0.444444   \n4                                 0.529412   \n\n   start_time_diff+start_time_diff_mean  start_time_diff+start_time_diff_std  \\\n0                          13200.580645                         24019.169180   \n1                          14614.928571                         25121.045843   \n2                          14614.928571                         25133.971996   \n3                          14917.611111                         23149.372122   \n4                          15795.117647                         26285.288742   \n\n   start_time_diff+max  start_time_diff+coefficient_of_variation  is_sa  \n0             105436.0                                  1.819554    0.0  \n1             105436.0                                  1.718862    0.0  \n2             105436.0                                  1.719746    0.0  \n3              74415.0                                  1.551815    1.0  \n4              74415.0                                  1.664140    1.0  \n\n[5 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>call_duration+sum</th>\n      <th>call_duration+mean</th>\n      <th>call_duration+max</th>\n      <th>call_duration+std</th>\n      <th>call_duration+quantile_25</th>\n      <th>call_duration+quantile_50</th>\n      <th>call_duration+quantile_75</th>\n      <th>cfee+sum</th>\n      <th>cfee+std</th>\n      <th>cfee+mean</th>\n      <th>...</th>\n      <th>other_party+account_person_num</th>\n      <th>other_party+called_diff_home_code</th>\n      <th>a_serv_type+call_num</th>\n      <th>a_serv_type+called_num</th>\n      <th>a_serv_type+call_called_normalized_diff</th>\n      <th>start_time_diff+start_time_diff_mean</th>\n      <th>start_time_diff+start_time_diff_std</th>\n      <th>start_time_diff+max</th>\n      <th>start_time_diff+coefficient_of_variation</th>\n      <th>is_sa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6369.0</td>\n      <td>68.483871</td>\n      <td>1129.0</td>\n      <td>128.481906</td>\n      <td>20.00</td>\n      <td>37.0</td>\n      <td>78.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>44.0</td>\n      <td>0.860215</td>\n      <td>62.0</td>\n      <td>31.0</td>\n      <td>0.333333</td>\n      <td>13200.580645</td>\n      <td>24019.169180</td>\n      <td>105436.0</td>\n      <td>1.819554</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6127.0</td>\n      <td>72.940476</td>\n      <td>1129.0</td>\n      <td>134.267612</td>\n      <td>21.75</td>\n      <td>41.0</td>\n      <td>78.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>40.0</td>\n      <td>0.869048</td>\n      <td>55.0</td>\n      <td>29.0</td>\n      <td>0.309524</td>\n      <td>14614.928571</td>\n      <td>25121.045843</td>\n      <td>105436.0</td>\n      <td>1.718862</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6067.0</td>\n      <td>72.226190</td>\n      <td>1129.0</td>\n      <td>134.270752</td>\n      <td>21.75</td>\n      <td>41.0</td>\n      <td>78.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>41.0</td>\n      <td>0.869048</td>\n      <td>55.0</td>\n      <td>29.0</td>\n      <td>0.309524</td>\n      <td>14614.928571</td>\n      <td>25133.971996</td>\n      <td>105436.0</td>\n      <td>1.719746</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>375.0</td>\n      <td>20.833333</td>\n      <td>156.0</td>\n      <td>39.165186</td>\n      <td>3.50</td>\n      <td>8.0</td>\n      <td>13.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6.0</td>\n      <td>0.333333</td>\n      <td>13.0</td>\n      <td>5.0</td>\n      <td>0.444444</td>\n      <td>14917.611111</td>\n      <td>23149.372122</td>\n      <td>74415.0</td>\n      <td>1.551815</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>370.0</td>\n      <td>21.764706</td>\n      <td>156.0</td>\n      <td>40.164551</td>\n      <td>3.00</td>\n      <td>8.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6.0</td>\n      <td>0.352941</td>\n      <td>13.0</td>\n      <td>4.0</td>\n      <td>0.529412</td>\n      <td>15795.117647</td>\n      <td>26285.288742</td>\n      <td>74415.0</td>\n      <td>1.664140</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.654777Z",
     "start_time": "2024-11-02T02:42:04.579533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       call_duration+sum  call_duration+mean  call_duration+max  \\\ncount        3859.000000         3859.000000        3859.000000   \nmean        10406.521897           98.551688        1019.947396   \nstd         12760.882888           92.222400        1033.736896   \nmin             9.000000            7.500000           9.000000   \n25%          2636.000000           49.842071         327.500000   \n50%          6210.000000           74.507692         702.000000   \n75%         13446.500000          114.641912        1437.000000   \nmax        148852.000000         1585.351351       12031.000000   \n\n       call_duration+std  call_duration+quantile_25  \\\ncount        3859.000000                3859.000000   \nmean          161.949909                  22.429386   \nstd           173.551887                  16.044677   \nmin             0.000000                   3.000000   \n25%            58.460836                  15.000000   \n50%           108.303908                  20.000000   \n75%           206.710735                  25.250000   \nmax          2985.972392                 557.500000   \n\n       call_duration+quantile_50  call_duration+quantile_75      cfee+sum  \\\ncount                3859.000000                3859.000000   3859.000000   \nmean                   44.574112                 101.245400    118.738015   \nstd                    37.889841                 113.887055    700.271881   \nmin                     7.500000                   8.750000      0.000000   \n25%                    28.000000                  54.000000      0.000000   \n50%                    37.500000                  75.500000      0.000000   \n75%                    50.000000                 111.000000      0.000000   \nmax                  1078.000000                2462.750000  13060.000000   \n\n          cfee+std    cfee+mean  ...  other_party+account_person_num  \\\ncount  3859.000000  3859.000000  ...                     3859.000000   \nmean      2.351438     1.248072  ...                       35.999223   \nstd      21.844227    22.155837  ...                       58.313255   \nmin       0.000000     0.000000  ...                        1.000000   \n25%       0.000000     0.000000  ...                       10.000000   \n50%       0.000000     0.000000  ...                       21.000000   \n75%       0.000000     0.000000  ...                       42.000000   \nmax    1158.022021  1345.500000  ...                     1182.000000   \n\n       other_party+called_diff_home_code  a_serv_type+call_num  \\\ncount                        3859.000000           3859.000000   \nmean                            0.723654             61.325214   \nstd                             0.163836             84.329243   \nmin                             0.020408              0.000000   \n25%                             0.651446             15.000000   \n50%                             0.755814             36.000000   \n75%                             0.833333             74.000000   \nmax                             1.000000           1188.000000   \n\n       a_serv_type+called_num  a_serv_type+call_called_normalized_diff  \\\ncount             3859.000000                              3859.000000   \nmean                56.421353                                -0.020429   \nstd                 65.668060                                 0.383948   \nmin                  0.000000                                -1.000000   \n25%                 17.000000                                -0.262935   \n50%                 37.000000                                -0.012658   \n75%                 70.000000                                 0.201156   \nmax                918.000000                                 1.000000   \n\n       start_time_diff+start_time_diff_mean  \\\ncount                           3859.000000   \nmean                           26362.870725   \nstd                            34537.860324   \nmin                                0.000000   \n25%                             8164.690789   \n50%                            15687.100000   \n75%                            30406.071646   \nmax                           419491.666667   \n\n       start_time_diff+start_time_diff_std  start_time_diff+max  \\\ncount                          3859.000000         3.859000e+03   \nmean                          41034.896147         1.675445e+05   \nstd                           50357.713939         1.385666e+05   \nmin                               0.000000         0.000000e+00   \n25%                           15486.485752         7.947500e+04   \n50%                           24991.270244         1.254000e+05   \n75%                           46539.805125         2.033185e+05   \nmax                          724139.196063         1.255654e+06   \n\n       start_time_diff+coefficient_of_variation        is_sa  \ncount                               3859.000000  3859.000000  \nmean                                   1.821371     0.099508  \nstd                                    0.744165     0.299381  \nmin                                    0.000000     0.000000  \n25%                                    1.424679     0.000000  \n50%                                    1.694879     0.000000  \n75%                                    2.030944     0.000000  \nmax                                   14.622485     1.000000  \n\n[8 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>call_duration+sum</th>\n      <th>call_duration+mean</th>\n      <th>call_duration+max</th>\n      <th>call_duration+std</th>\n      <th>call_duration+quantile_25</th>\n      <th>call_duration+quantile_50</th>\n      <th>call_duration+quantile_75</th>\n      <th>cfee+sum</th>\n      <th>cfee+std</th>\n      <th>cfee+mean</th>\n      <th>...</th>\n      <th>other_party+account_person_num</th>\n      <th>other_party+called_diff_home_code</th>\n      <th>a_serv_type+call_num</th>\n      <th>a_serv_type+called_num</th>\n      <th>a_serv_type+call_called_normalized_diff</th>\n      <th>start_time_diff+start_time_diff_mean</th>\n      <th>start_time_diff+start_time_diff_std</th>\n      <th>start_time_diff+max</th>\n      <th>start_time_diff+coefficient_of_variation</th>\n      <th>is_sa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>...</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n      <td>3.859000e+03</td>\n      <td>3859.000000</td>\n      <td>3859.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10406.521897</td>\n      <td>98.551688</td>\n      <td>1019.947396</td>\n      <td>161.949909</td>\n      <td>22.429386</td>\n      <td>44.574112</td>\n      <td>101.245400</td>\n      <td>118.738015</td>\n      <td>2.351438</td>\n      <td>1.248072</td>\n      <td>...</td>\n      <td>35.999223</td>\n      <td>0.723654</td>\n      <td>61.325214</td>\n      <td>56.421353</td>\n      <td>-0.020429</td>\n      <td>26362.870725</td>\n      <td>41034.896147</td>\n      <td>1.675445e+05</td>\n      <td>1.821371</td>\n      <td>0.099508</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>12760.882888</td>\n      <td>92.222400</td>\n      <td>1033.736896</td>\n      <td>173.551887</td>\n      <td>16.044677</td>\n      <td>37.889841</td>\n      <td>113.887055</td>\n      <td>700.271881</td>\n      <td>21.844227</td>\n      <td>22.155837</td>\n      <td>...</td>\n      <td>58.313255</td>\n      <td>0.163836</td>\n      <td>84.329243</td>\n      <td>65.668060</td>\n      <td>0.383948</td>\n      <td>34537.860324</td>\n      <td>50357.713939</td>\n      <td>1.385666e+05</td>\n      <td>0.744165</td>\n      <td>0.299381</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>9.000000</td>\n      <td>7.500000</td>\n      <td>9.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>7.500000</td>\n      <td>8.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.020408</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2636.000000</td>\n      <td>49.842071</td>\n      <td>327.500000</td>\n      <td>58.460836</td>\n      <td>15.000000</td>\n      <td>28.000000</td>\n      <td>54.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>10.000000</td>\n      <td>0.651446</td>\n      <td>15.000000</td>\n      <td>17.000000</td>\n      <td>-0.262935</td>\n      <td>8164.690789</td>\n      <td>15486.485752</td>\n      <td>7.947500e+04</td>\n      <td>1.424679</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6210.000000</td>\n      <td>74.507692</td>\n      <td>702.000000</td>\n      <td>108.303908</td>\n      <td>20.000000</td>\n      <td>37.500000</td>\n      <td>75.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>21.000000</td>\n      <td>0.755814</td>\n      <td>36.000000</td>\n      <td>37.000000</td>\n      <td>-0.012658</td>\n      <td>15687.100000</td>\n      <td>24991.270244</td>\n      <td>1.254000e+05</td>\n      <td>1.694879</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13446.500000</td>\n      <td>114.641912</td>\n      <td>1437.000000</td>\n      <td>206.710735</td>\n      <td>25.250000</td>\n      <td>50.000000</td>\n      <td>111.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>42.000000</td>\n      <td>0.833333</td>\n      <td>74.000000</td>\n      <td>70.000000</td>\n      <td>0.201156</td>\n      <td>30406.071646</td>\n      <td>46539.805125</td>\n      <td>2.033185e+05</td>\n      <td>2.030944</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>148852.000000</td>\n      <td>1585.351351</td>\n      <td>12031.000000</td>\n      <td>2985.972392</td>\n      <td>557.500000</td>\n      <td>1078.000000</td>\n      <td>2462.750000</td>\n      <td>13060.000000</td>\n      <td>1158.022021</td>\n      <td>1345.500000</td>\n      <td>...</td>\n      <td>1182.000000</td>\n      <td>1.000000</td>\n      <td>1188.000000</td>\n      <td>918.000000</td>\n      <td>1.000000</td>\n      <td>419491.666667</td>\n      <td>724139.196063</td>\n      <td>1.255654e+06</td>\n      <td>14.622485</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.715144Z",
     "start_time": "2024-11-02T02:42:04.656780Z"
    }
   },
   "outputs": [],
   "source": [
    "all_set = pd.concat([train_set, test_set, valid_set], axis=0).reset_index(drop=True)\n",
    "labeled_data_len = train_set.shape[0] + test_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.729882Z",
     "start_time": "2024-11-02T02:42:04.717181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((3859, 40), (129732, 40), (13005, 40), (146596, 40))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape, train_set.shape, valid_set.shape, all_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据增强\n",
    "NOTE: 因为有些聚类不能处理字符串数据，所以在这里把转换为数值格式的数据进行处理，而不是一开始针对原数据做增强"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:42:04.745282Z",
     "start_time": "2024-11-02T02:42:04.734946Z"
    }
   },
   "outputs": [],
   "source": [
    "NO_SMOTE = True\n",
    "NO_ADASYN = True\n",
    "CLUSTER = True\n",
    "SHUFFLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:44:32.105603Z",
     "start_time": "2024-11-02T02:44:32.073236Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "def cluster_oversample(data, label_col='is_sa', cluster_method='kmeans++', k=5, sample_ratio=1.0):\n",
    "    # 1. 提取标签为0和1的样本\n",
    "    normal_data = data[data[label_col] == 0].drop(columns=[label_col])\n",
    "    malicious_count = len(data[data[label_col] == 1])\n",
    "    normal_count = len(normal_data)\n",
    "    \n",
    "    # 计算需要生成的新样本总数并分成四份\n",
    "    target_sample_count = int((normal_count - malicious_count) * sample_ratio)\n",
    "    samples_per_cluster_per_method = (target_sample_count // 4) // k  # 每种方法生成的样本数量\n",
    "    new_samples = []\n",
    "\n",
    "    # 2. 聚类\n",
    "    if cluster_method == 'kmeans++':\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=0)\n",
    "        labels = kmeans.fit_predict(normal_data)\n",
    "        clusters = {i: normal_data[labels == i] for i in range(k)}\n",
    "    elif cluster_method == 'other_methods':\n",
    "        pass\n",
    "        # TODO: 其他聚类方法\n",
    "\n",
    "    # 3. 逐方法生成新样本\n",
    "\n",
    "    # 方法1：聚类中心加噪声\n",
    "    for cluster_id, cluster_data in clusters.items():\n",
    "        cluster_center = cluster_data.mean(axis=0).values\n",
    "        noise = np.random.normal(0, 0.01, size=(samples_per_cluster_per_method, cluster_data.shape[1]))\n",
    "        synthetic_samples = cluster_center + noise\n",
    "        synthetic_samples_df = pd.DataFrame(synthetic_samples, columns=normal_data.columns)\n",
    "        new_samples.append(synthetic_samples_df)\n",
    "    \n",
    "    # 方法2：聚类内样本插值 (C-SMOTE)\n",
    "    for cluster_id, cluster_data in clusters.items():\n",
    "        nn = NearestNeighbors(n_neighbors=2).fit(cluster_data)\n",
    "        interpolated_samples = []\n",
    "        for _ in range(samples_per_cluster_per_method):\n",
    "            sample_idx = np.random.randint(0, len(cluster_data))\n",
    "            sample_point = cluster_data.iloc[[sample_idx]]\n",
    "            _, neighbors = nn.kneighbors(sample_point)  # 直接传入 DataFrame 保持一致性\n",
    "            neighbor_idx = neighbors[0, 1]\n",
    "            sample_a = cluster_data.iloc[sample_idx].values\n",
    "            sample_b = cluster_data.iloc[neighbor_idx].values\n",
    "            interpolated_sample = sample_a + np.random.rand() * (sample_b - sample_a)\n",
    "            interpolated_samples.append(interpolated_sample)\n",
    "        interpolated_samples_df = pd.DataFrame(interpolated_samples, columns=normal_data.columns)\n",
    "        new_samples.append(interpolated_samples_df)\n",
    "\n",
    "    # 方法3：自适应聚类过采样 (ACO)\n",
    "    for cluster_id, cluster_data in clusters.items():\n",
    "        cluster_center = cluster_data.mean(axis=0).values\n",
    "        noise = np.random.normal(0, 0.01, size=(samples_per_cluster_per_method, cluster_data.shape[1]))\n",
    "        adaptive_samples = cluster_center + noise\n",
    "        adaptive_samples_df = pd.DataFrame(adaptive_samples, columns=normal_data.columns)\n",
    "        new_samples.append(adaptive_samples_df)\n",
    "\n",
    "    # 方法4：基于聚类的 SMOTE (Cluster SMOTE)\n",
    "    for cluster_id, cluster_data in clusters.items():\n",
    "        smote_samples = []\n",
    "        for _ in range(samples_per_cluster_per_method):\n",
    "            sample_pair = cluster_data.sample(2)\n",
    "            interpolated_sample = sample_pair.mean().values.reshape(1, -1)\n",
    "            smote_samples.append(interpolated_sample[0])\n",
    "        smote_samples_df = pd.DataFrame(smote_samples, columns=normal_data.columns)\n",
    "        new_samples.append(smote_samples_df)\n",
    "\n",
    "    # 4. 合并生成的新样本并添加标签\n",
    "    new_samples = pd.concat(new_samples, ignore_index=True)\n",
    "    new_samples[label_col] = 1\n",
    "\n",
    "    # 打印新生成样本的总数\n",
    "    print(f\"Total new samples generated: {len(new_samples)} (Target: {target_sample_count})\")\n",
    "\n",
    "    return data, new_samples\n",
    "\n",
    "\n",
    "def fly_augmentation(train_set: pd.DataFrame, ratio: float, mode: str) -> pd.DataFrame:\n",
    "    # Calculate the sample size for each class based on the ratio\n",
    "    num_samples = int(ratio * len(train_set))\n",
    "    \n",
    "    # Separate the dataset by class\n",
    "    class_0 = train_set[train_set['is_sa'] == 0]\n",
    "    class_1 = train_set[train_set['is_sa'] == 1]\n",
    "    \n",
    "    # Sample from each class without replacement to avoid changing the total number of rows\n",
    "    sampled_class_0 = class_0.sample(n=num_samples, replace=False, random_state=42)\n",
    "    sampled_class_1 = class_1.sample(n=num_samples, replace=False, random_state=42)\n",
    "    \n",
    "    # Concatenate the samples from both classes\n",
    "    augmented_data = pd.concat([sampled_class_0, sampled_class_1])\n",
    "    \n",
    "    # Get feature columns (excluding the last column 'is_sa')\n",
    "    features = augmented_data.columns[:-1]\n",
    "    \n",
    "    # Mode \"intra-class\": Shuffle features within each class\n",
    "    if mode == \"intra-class\":\n",
    "        for col in features:\n",
    "            augmented_data.loc[augmented_data['is_sa'] == 0, col] = np.random.permutation(augmented_data[augmented_data['is_sa'] == 0][col].values)\n",
    "            augmented_data.loc[augmented_data['is_sa'] == 1, col] = np.random.permutation(augmented_data[augmented_data['is_sa'] == 1][col].values)\n",
    "    \n",
    "    # Mode \"inter-class\": Shuffle features between classes\n",
    "    elif mode == \"inter-class\":\n",
    "        for col in features:\n",
    "            combined_values = np.concatenate((augmented_data[augmented_data['is_sa'] == 0][col].values,\n",
    "                                              augmented_data[augmented_data['is_sa'] == 1][col].values))\n",
    "            shuffled_values = np.random.permutation(combined_values)\n",
    "            \n",
    "            # Split the shuffled values back into the two classes\n",
    "            augmented_data.loc[augmented_data['is_sa'] == 0, col] = shuffled_values[:num_samples]\n",
    "            augmented_data.loc[augmented_data['is_sa'] == 1, col] = shuffled_values[num_samples:]\n",
    "    \n",
    "    # Replace the sampled original data with the augmented data in train_set\n",
    "    train_set.update(augmented_data)\n",
    "    \n",
    "    return train_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\decdiff\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new samples generated: 52180 (Target: 52196)\n",
      "通过cluster sampling方法平衡正负样本后\n",
      "样本个数：181912; 正样本占50.00%; 负样本占50.00%\n"
     ]
    }
   ],
   "source": [
    "labeled_set, valid_set = all_set.iloc[:labeled_data_len].copy(), all_set.iloc[labeled_data_len:].copy()\n",
    "labeled_set.reset_index(drop=True, inplace=True)\n",
    "valid_set.reset_index(drop=True, inplace=True)\n",
    "# 有一些值在SMOTE后对数变换后为 NaN，需要删除这些数据\n",
    "print(labeled_set.isnull().sum().sum())\n",
    "labeled_set = labeled_set.dropna()\n",
    "print(labeled_set.isnull().sum().sum())\n",
    "assert valid_set.shape[0] == validation_features.shape[0]\n",
    "\n",
    "# 重新划分训练集和测试集\n",
    "if not ALL:\n",
    "    train_set, test_set = labeled_set.iloc[:train_len].copy(), labeled_set.iloc[train_len:].copy()\n",
    "    train_set.reset_index(drop=True, inplace=True)\n",
    "    test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # remove_columns = ['distance_distance_std', \"start_time_diff_start_time_diff_max\", \"distance_distance_quantile_75\"]\n",
    "\n",
    "    # remove_columns = [\"lfee_lfee_std\", \"lfee_lfee_mean\", 'call_duration_call_duration_max', \"distance_distance_quantile_50\", \"call_duration_call_duration_quantile_25\"]\n",
    "    # remove_columns = [\"7\", \"6\", \"lfee_lfee_mean\", \"hour_hour_std\", \"1\", \"call_duration_call_duration_quantile_75\", \"3\", \"cfee_cfee_std\", \"start_time_diff_start_time_diff_max\", \"call_duration_call_duration_max\", \"dayofweek_dayofweek_mode\", \"distance_distance_quantile_75\", \"cfee_cfee_mean\"] # , \"visit_area_code_visit_area_code_nunique\", \"visit_area_code_visit_area_code_nunique\"\n",
    "    # remove_columns = ['visit_area_code+nunique_cross_start_time_diff+max', \"distance+std\"]\n",
    "    # remove_columns = ['dayofweek+std', 'start_time_diff+max', 'distance+quantile_75', 'lfee+mean', 'lfee+std', 'lfee+sum', 'cfee+sum', '6', 'visit_area_code+nunique']\n",
    "    remove_columns = ['cfee+std', 'start_time_diff+start_time_diff_std', 'lfee+mean', 'lfee+sum', 'lfee+std']\n",
    "    train_set = train_set.drop(remove_columns, axis=1)\n",
    "    test_set = test_set.drop(remove_columns, axis=1)\n",
    "    valid_set = valid_set.drop(remove_columns, axis=1)\n",
    "    \n",
    "    # 对采样数据做 smote\n",
    "    if not NO_SMOTE:\n",
    "        smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "        X_train, y_train = smote.fit_resample(train_set.drop(['is_sa'], axis=1), train_set['is_sa'])\n",
    "        train_set = pd.concat([X_train, y_train], axis=1)\n",
    "        print('通过SMOTE方法平衡正负样本后')\n",
    "        n_sample = y_train.shape[0]\n",
    "        n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "        n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "        print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                        n_pos_sample / n_sample,\n",
    "                                                        n_neg_sample / n_sample))\n",
    "    elif not NO_ADASYN:\n",
    "        adasyn = ADASYN(random_state=42)\n",
    "        X_train, y_train = adasyn.fit_resample(train_set.drop(['is_sa'], axis=1), train_set['is_sa'])\n",
    "        train_set = pd.concat([X_train, y_train], axis=1)\n",
    "        print('通过ADASYN方法平衡正负样本后')\n",
    "        n_sample = y_train.shape[0]\n",
    "        n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "        n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "        print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                        n_pos_sample / n_sample,\n",
    "                                                        n_neg_sample / n_sample))\n",
    "    elif CLUSTER:\n",
    "        train_set, new_sample = cluster_oversample(train_set, label_col='is_sa', cluster_method='kmeans++', k=5, sample_ratio=1.0)\n",
    "        if SHUFFLE:\n",
    "            # 只针对原数据做shuffle\n",
    "            train_set = fly_augmentation(train_set, ratio=0.01, mode='intra-class')\n",
    "        train_set = pd.concat([train_set, new_sample], ignore_index=True)\n",
    "        print('通过cluster sampling方法平衡正负样本后')\n",
    "        n_sample = len(train_set)\n",
    "        n_pos_sample = len(train_set[train_set['is_sa'] == 1])\n",
    "        n_neg_sample = len(train_set[train_set['is_sa'] == 0])\n",
    "        print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                        n_pos_sample / n_sample,\n",
    "                                                        n_neg_sample / n_sample))\n",
    "    assert train_set.shape[1] == test_set.shape[1] == valid_set.shape[1]\n",
    "else:\n",
    "    if not NO_SMOTE:\n",
    "        # BUG:\n",
    "        # 对 all_set 做 smote\n",
    "        smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "        X_train, y_train = smote.fit_resample(labeled_set.drop(['is_sa'], axis=1), labeled_set['is_sa'])\n",
    "        labeled_set = pd.concat([X_train, y_train], axis=1)\n",
    "        print('通过SMOTE方法平衡正负样本后')\n",
    "        n_sample = y_train.shape[0]\n",
    "        n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "        n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "        print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                        n_pos_sample / n_sample,\n",
    "                                                        n_neg_sample / n_sample))\n",
    "    elif not NO_ADASYN:\n",
    "        adasyn = ADASYN(random_state=42)\n",
    "        X_train, y_train = adasyn.fit_resample(labeled_set.drop(['is_sa'], axis=1), labeled_set['is_sa'])\n",
    "        labeled_set = pd.concat([X_train, y_train], axis=1)\n",
    "        print('通过ADASYN方法平衡正负样本后')\n",
    "        n_sample = y_train.shape[0]\n",
    "        n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "        n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "        print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                        n_pos_sample / n_sample,\n",
    "                                                        n_neg_sample / n_sample))\n",
    "    elif CLUSTER:\n",
    "        labeled_set, new_sample = cluster_oversample(labeled_set, label_col='is_sa', cluster_method='kmeans++', k=5, sample_ratio=1.0)\n",
    "        if SHUFFLE:\n",
    "            # 只针对原数据做shuffle\n",
    "            labeled_set = fly_augmentation(labeled_set, ratio=0.01, mode='intra-class')\n",
    "        labeled_set = pd.concat([labeled_set, new_sample], ignore_index=True)\n",
    "        print('通过cluster sampling方法平衡正负样本后')\n",
    "        n_sample = len(train_set)\n",
    "        n_pos_sample = len(train_set[train_set['is_sa'] == 1])\n",
    "        n_neg_sample = len(train_set[train_set['is_sa'] == 0])\n",
    "        print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                        n_pos_sample / n_sample,\n",
    "                                                        n_neg_sample / n_sample))\n",
    "        \n",
    "    remove_columns = ['0', '1', '2', '3', '4', '5', '6', '7', 'cfee+std', 'start_time_diff+start_time_diff_std', 'lfee+mean', 'lfee+sum', 'lfee+std']\n",
    "    labeled_set = labeled_set.drop(remove_columns, axis=1)\n",
    "    valid_set = valid_set.drop(remove_columns, axis=1)\n",
    "    test_set = test_set.drop(remove_columns, axis=1)\n",
    "    assert labeled_set.shape[1] == valid_set.shape[1] == test_set.shape[1]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:46:39.900972Z",
     "start_time": "2024-11-02T02:45:56.606700Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        call_duration+sum  call_duration+mean  call_duration+max  \\\n181907              792.5           52.861111              321.5   \n181908             1320.5          336.950980              856.5   \n181909             1060.5           75.750000              215.0   \n181910             4024.5          244.291667             1286.0   \n181911             1263.5           52.674731              180.0   \n\n        call_duration+std  call_duration+quantile_25  \\\n181907          82.759453                     15.125   \n181908         426.525167                     90.500   \n181909          68.400737                     28.750   \n181910         376.860831                     30.125   \n181911          43.160178                     18.500   \n\n        call_duration+quantile_50  call_duration+quantile_75  cfee+sum  \\\n181907                      24.50                     43.125       0.0   \n181908                     174.50                    495.250       0.0   \n181909                      48.50                     99.125       0.0   \n181910                      70.75                    231.750       0.0   \n181911                      43.00                     75.125       0.0   \n\n        cfee+mean  hour+mean  ...  open_datetime+open_count  \\\n181907        0.0  15.019841  ...                       1.0   \n181908        0.0  14.333333  ...                       1.0   \n181909        0.0  13.571429  ...                       1.0   \n181910        0.0  13.760417  ...                       1.0   \n181911        0.0  15.822581  ...                       1.0   \n\n        other_party+account_person_num  other_party+called_diff_home_code  \\\n181907                             7.0                           0.634921   \n181908                             4.0                           0.627451   \n181909                             9.5                           0.750000   \n181910                             9.0                           0.921131   \n181911                            12.0                           0.509857   \n\n        a_serv_type+call_num  a_serv_type+called_num  \\\n181907                   7.0                     9.0   \n181908                   2.5                     7.5   \n181909                   2.0                    12.0   \n181910                  14.5                     4.0   \n181911                   1.5                    23.0   \n\n        a_serv_type+call_called_normalized_diff  \\\n181907                                -0.158730   \n181908                                -0.705882   \n181909                                -0.714286   \n181910                                 0.589286   \n181911                                -0.833333   \n\n        start_time_diff+start_time_diff_mean  start_time_diff+max  \\\n181907                          78023.896825             491457.5   \n181908                         155626.617647             479804.0   \n181909                          80028.678571             556940.0   \n181910                          56870.799107             414895.5   \n181911                          53228.601254             492249.5   \n\n        start_time_diff+coefficient_of_variation  is_sa  \n181907                                  1.678701    1.0  \n181908                                  1.496259    1.0  \n181909                                  1.882867    1.0  \n181910                                  1.820382    1.0  \n181911                                  2.069673    1.0  \n\n[5 rows x 35 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>call_duration+sum</th>\n      <th>call_duration+mean</th>\n      <th>call_duration+max</th>\n      <th>call_duration+std</th>\n      <th>call_duration+quantile_25</th>\n      <th>call_duration+quantile_50</th>\n      <th>call_duration+quantile_75</th>\n      <th>cfee+sum</th>\n      <th>cfee+mean</th>\n      <th>hour+mean</th>\n      <th>...</th>\n      <th>open_datetime+open_count</th>\n      <th>other_party+account_person_num</th>\n      <th>other_party+called_diff_home_code</th>\n      <th>a_serv_type+call_num</th>\n      <th>a_serv_type+called_num</th>\n      <th>a_serv_type+call_called_normalized_diff</th>\n      <th>start_time_diff+start_time_diff_mean</th>\n      <th>start_time_diff+max</th>\n      <th>start_time_diff+coefficient_of_variation</th>\n      <th>is_sa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>181907</th>\n      <td>792.5</td>\n      <td>52.861111</td>\n      <td>321.5</td>\n      <td>82.759453</td>\n      <td>15.125</td>\n      <td>24.50</td>\n      <td>43.125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.019841</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>0.634921</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>-0.158730</td>\n      <td>78023.896825</td>\n      <td>491457.5</td>\n      <td>1.678701</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>181908</th>\n      <td>1320.5</td>\n      <td>336.950980</td>\n      <td>856.5</td>\n      <td>426.525167</td>\n      <td>90.500</td>\n      <td>174.50</td>\n      <td>495.250</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.333333</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.627451</td>\n      <td>2.5</td>\n      <td>7.5</td>\n      <td>-0.705882</td>\n      <td>155626.617647</td>\n      <td>479804.0</td>\n      <td>1.496259</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>181909</th>\n      <td>1060.5</td>\n      <td>75.750000</td>\n      <td>215.0</td>\n      <td>68.400737</td>\n      <td>28.750</td>\n      <td>48.50</td>\n      <td>99.125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.571429</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>9.5</td>\n      <td>0.750000</td>\n      <td>2.0</td>\n      <td>12.0</td>\n      <td>-0.714286</td>\n      <td>80028.678571</td>\n      <td>556940.0</td>\n      <td>1.882867</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>181910</th>\n      <td>4024.5</td>\n      <td>244.291667</td>\n      <td>1286.0</td>\n      <td>376.860831</td>\n      <td>30.125</td>\n      <td>70.75</td>\n      <td>231.750</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.760417</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.921131</td>\n      <td>14.5</td>\n      <td>4.0</td>\n      <td>0.589286</td>\n      <td>56870.799107</td>\n      <td>414895.5</td>\n      <td>1.820382</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>181911</th>\n      <td>1263.5</td>\n      <td>52.674731</td>\n      <td>180.0</td>\n      <td>43.160178</td>\n      <td>18.500</td>\n      <td>43.00</td>\n      <td>75.125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.822581</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>12.0</td>\n      <td>0.509857</td>\n      <td>1.5</td>\n      <td>23.0</td>\n      <td>-0.833333</td>\n      <td>53228.601254</td>\n      <td>492249.5</td>\n      <td>2.069673</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:47:05.516829Z",
     "start_time": "2024-11-02T02:47:05.490939Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       call_duration+sum  call_duration+mean  call_duration+max  \\\n52175              792.5           52.861111              321.5   \n52176             1320.5          336.950980              856.5   \n52177             1060.5           75.750000              215.0   \n52178             4024.5          244.291667             1286.0   \n52179             1263.5           52.674731              180.0   \n\n       call_duration+std  call_duration+quantile_25  \\\n52175          82.759453                     15.125   \n52176         426.525167                     90.500   \n52177          68.400737                     28.750   \n52178         376.860831                     30.125   \n52179          43.160178                     18.500   \n\n       call_duration+quantile_50  call_duration+quantile_75  cfee+sum  \\\n52175                      24.50                     43.125       0.0   \n52176                     174.50                    495.250       0.0   \n52177                      48.50                     99.125       0.0   \n52178                      70.75                    231.750       0.0   \n52179                      43.00                     75.125       0.0   \n\n       cfee+mean  hour+mean  ...  open_datetime+open_count  \\\n52175        0.0  15.019841  ...                       1.0   \n52176        0.0  14.333333  ...                       1.0   \n52177        0.0  13.571429  ...                       1.0   \n52178        0.0  13.760417  ...                       1.0   \n52179        0.0  15.822581  ...                       1.0   \n\n       other_party+account_person_num  other_party+called_diff_home_code  \\\n52175                             7.0                           0.634921   \n52176                             4.0                           0.627451   \n52177                             9.5                           0.750000   \n52178                             9.0                           0.921131   \n52179                            12.0                           0.509857   \n\n       a_serv_type+call_num  a_serv_type+called_num  \\\n52175                   7.0                     9.0   \n52176                   2.5                     7.5   \n52177                   2.0                    12.0   \n52178                  14.5                     4.0   \n52179                   1.5                    23.0   \n\n       a_serv_type+call_called_normalized_diff  \\\n52175                                -0.158730   \n52176                                -0.705882   \n52177                                -0.714286   \n52178                                 0.589286   \n52179                                -0.833333   \n\n       start_time_diff+start_time_diff_mean  start_time_diff+max  \\\n52175                          78023.896825             491457.5   \n52176                         155626.617647             479804.0   \n52177                          80028.678571             556940.0   \n52178                          56870.799107             414895.5   \n52179                          53228.601254             492249.5   \n\n       start_time_diff+coefficient_of_variation  is_sa  \n52175                                  1.678701      1  \n52176                                  1.496259      1  \n52177                                  1.882867      1  \n52178                                  1.820382      1  \n52179                                  2.069673      1  \n\n[5 rows x 35 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>call_duration+sum</th>\n      <th>call_duration+mean</th>\n      <th>call_duration+max</th>\n      <th>call_duration+std</th>\n      <th>call_duration+quantile_25</th>\n      <th>call_duration+quantile_50</th>\n      <th>call_duration+quantile_75</th>\n      <th>cfee+sum</th>\n      <th>cfee+mean</th>\n      <th>hour+mean</th>\n      <th>...</th>\n      <th>open_datetime+open_count</th>\n      <th>other_party+account_person_num</th>\n      <th>other_party+called_diff_home_code</th>\n      <th>a_serv_type+call_num</th>\n      <th>a_serv_type+called_num</th>\n      <th>a_serv_type+call_called_normalized_diff</th>\n      <th>start_time_diff+start_time_diff_mean</th>\n      <th>start_time_diff+max</th>\n      <th>start_time_diff+coefficient_of_variation</th>\n      <th>is_sa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52175</th>\n      <td>792.5</td>\n      <td>52.861111</td>\n      <td>321.5</td>\n      <td>82.759453</td>\n      <td>15.125</td>\n      <td>24.50</td>\n      <td>43.125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.019841</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>0.634921</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>-0.158730</td>\n      <td>78023.896825</td>\n      <td>491457.5</td>\n      <td>1.678701</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>52176</th>\n      <td>1320.5</td>\n      <td>336.950980</td>\n      <td>856.5</td>\n      <td>426.525167</td>\n      <td>90.500</td>\n      <td>174.50</td>\n      <td>495.250</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.333333</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.627451</td>\n      <td>2.5</td>\n      <td>7.5</td>\n      <td>-0.705882</td>\n      <td>155626.617647</td>\n      <td>479804.0</td>\n      <td>1.496259</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>52177</th>\n      <td>1060.5</td>\n      <td>75.750000</td>\n      <td>215.0</td>\n      <td>68.400737</td>\n      <td>28.750</td>\n      <td>48.50</td>\n      <td>99.125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.571429</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>9.5</td>\n      <td>0.750000</td>\n      <td>2.0</td>\n      <td>12.0</td>\n      <td>-0.714286</td>\n      <td>80028.678571</td>\n      <td>556940.0</td>\n      <td>1.882867</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>52178</th>\n      <td>4024.5</td>\n      <td>244.291667</td>\n      <td>1286.0</td>\n      <td>376.860831</td>\n      <td>30.125</td>\n      <td>70.75</td>\n      <td>231.750</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.760417</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.921131</td>\n      <td>14.5</td>\n      <td>4.0</td>\n      <td>0.589286</td>\n      <td>56870.799107</td>\n      <td>414895.5</td>\n      <td>1.820382</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>52179</th>\n      <td>1263.5</td>\n      <td>52.674731</td>\n      <td>180.0</td>\n      <td>43.160178</td>\n      <td>18.500</td>\n      <td>43.00</td>\n      <td>75.125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.822581</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>12.0</td>\n      <td>0.509857</td>\n      <td>1.5</td>\n      <td>23.0</td>\n      <td>-0.833333</td>\n      <td>53228.601254</td>\n      <td>492249.5</td>\n      <td>2.069673</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sample.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T02:47:12.060394Z",
     "start_time": "2024-11-02T02:47:12.031232Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T16:32:17.817224Z",
     "start_time": "2024-11-01T16:32:17.802747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((181912, 35), (3859, 35), (13005, 35))"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape, test_set.shape, valid_set.shape\n",
    "# ((129732, 35), (3859, 35), (13005, 35))\n",
    "# ((181928, 35), (3859, 35), (13005, 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 autogluon 训练\n",
    "from autogluon.tabular import TabularPredictor\n",
    "# import ray\n",
    "# 使用防止过拟合的超参数\n",
    "# hyperparameters = {\n",
    "#     'GBM': {'lambda_l1': 1e-2, 'lambda_l2': 1e-2},\n",
    "#     'FASTAI': {'dropout_prob': 0.2}\n",
    "# }\n",
    "# ray.shutdown()\n",
    "# ray.init(include_dashboard=True, object_store_memory=10**9)  # Increase object store memory\n",
    "\n",
    "# 输入数据X_train, y_train\n",
    "if not ALL:\n",
    "    # 交叉验证训练\n",
    "    model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(train_set, presets='medium_quality', time_limit=3600)\n",
    "    # , excluded_model_types=['KNN']\n",
    "    # model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(train_set, presets='best_quality', time_limit=3600)\n",
    "else:\n",
    "    model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(labeled_set, presets='best_quality', num_bag_folds=10, time_limit=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL:\n",
    "    print(model.evaluate(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importance(test_set if not ALL else labeled_set)\n",
    "print(feature_importance)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaderboard\n",
    "if not ALL:\n",
    "    leaderboard = model.leaderboard(test_set, silent=True)\n",
    "    print(leaderboard)\n",
    "else:\n",
    "    leaderboard = model.leaderboard(labeled_set, silent=True)\n",
    "    print(leaderboard)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 假设 model 已经训练好，并且 test_set 已经定义\n",
    "if not ALL:\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_true = test_set['is_sa']\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # 可视化混淆矩阵\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"../vis\", exist_ok=True)\n",
    "    plt.savefig(\"../vis/confusion_matrix.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型决策阈值微调\n",
    "threadhold = 0.2\n",
    "if not ALL:\n",
    "    y_pred_proba = model.predict_proba(test_set)\n",
    "    # print(y_pred_proba)\n",
    "    y_pred = (y_pred_proba.iloc[:, 1] > threadhold).astype(int)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "y_validation_pred = model.predict(valid_set.drop('is_sa', axis=1))\n",
    "\n",
    "# 将预测结果与 msisdn 对应起来\n",
    "validation_results = validation_features[['msisdn']].copy()\n",
    "validation_results['is_sa'] = y_validation_pred.astype(int)\n",
    "\n",
    "print(validation_results.describe())\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "import time\n",
    "time_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "file_name = './valid_large_data_{}.csv'.format(time_str) if ALL else './valid_small_data_{}.csv'.format(time_str)\n",
    "validation_results.to_csv(file_name, index=False)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# # 自动调整最佳阈值\n",
    "# if not ALL:\n",
    "#     y_true = test_set['is_sa']\n",
    "#     y_pred_proba = model.predict_proba(test_set)\n",
    "\n",
    "#     thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "#     f1_scores = []\n",
    "\n",
    "#     for threshold in thresholds:\n",
    "#         y_pred = (y_pred_proba.iloc[:, 1] >= threshold).astype(int)\n",
    "#         f1 = f1_score(y_true, y_pred)\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#     best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "#     print(f'最佳阈值：{best_threshold}')\n",
    "#     print(f'最佳 F1 分数：{max(f1_scores)}')\n",
    "\n",
    "#     # 使用最佳阈值进行预测\n",
    "#     y_pred = (y_pred_proba.iloc[:, 1] >= best_threshold).astype(int)\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 阈值微调版结果\n",
    "# best_threshold = 0.3\n",
    "# # 使用最佳决策阈值进行预测\n",
    "# y_validation_pred_proba = model.predict_proba(valid_set.drop('is_sa', axis=1))\n",
    "# y_validation_pred = (y_validation_pred_proba.iloc[:, 1] >= best_threshold).astype(int)\n",
    "\n",
    "# # 将预测结果与 msisdn 对应起来\n",
    "# validation_results = validation_features[['msisdn']].copy()\n",
    "# validation_results['is_sa'] = y_validation_pred.astype(int)\n",
    "\n",
    "# print(validation_results.describe())\n",
    "\n",
    "# # 保存结果到CSV文件\n",
    "# import time\n",
    "# time_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "# file_name = './valid_large_data_{}.csv'.format(time_str) if ALL else './valid_small_data_{}.csv'.format(time_str)\n",
    "# validation_results.to_csv(file_name, index=False)\n",
    "# print(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
